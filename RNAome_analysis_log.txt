module load python3
mkdir alignment
module load hisat2 samtools trimmomatic java
module load gcc cutadapt
#make sure no space in mirna reads

###skip this software###-----------trimming using trimmomatic-----------------------
# for file in *_R1.fastq.gz; do echo ${file%R*}; trimmomatic PE ${file%R*}R1.fastq.gz ${file%R*}R2.fastq.gz -baseout ${file%R*}R.fq.gz ILLUMINACLIP:/ufrc/wang/luoziliang/nodulation/genome/adaptor.1.19.fasta:2:30:10:8:true LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:20; done

#after trimming, concatenate the paired and unpaired files respectively
#for f in *1P.fq.gz; do echo ${f}; mv ${f} ${f%%_*}Paire1_S.fq.gz; mv ${f%R*}R_2P.fq.gz ${f%%_*}Paire2_S.fq.gz; cat ${f%R*}R_1U.fq.gz ${f%R*}R_2U.fq.gz>${f%%_*}Unpaired_S.fq.gz; done
#separate read length, default is 30bp
#python fastq_length_filter.py
#!!!for paired reads, they don't necessarily have same length after trimming. Therefore, cannot filter by length before trimming.
#!!! the unequal reads number after filter cannot be input into hisat2 for alignment.
#for f in *Paire1_long_reads.fastq ; do echo Sample${f%P*}; hisat2 -p 4 -x /ufrc/wang/luoziliang/nodulation/genome/tif_gnm -1 ${f} -2 ${f%P*}Paire2_long_reads.fastq -U ${f%P*}Unpaired_long_reads.fastq -S alignment/sample${f%P*}.sam; done
-----------------------------------------------------------------

-----------------trimming using cutadapt-------------------------
cutadapt -a file:adaptor.1.19.fasta -A file:adaptor.1.19.fasta -j 4 -m 15 -e 0.2 -o ./evaluation_cutadapt/15_S27_1P.fastq.gz -p ./evaluation_cutadapt/15_S27_2P.fastq.gz 15_S27_R1.fastq.gz 15_S27_R2.fastq.gz
# -j: thread used
# -m: min length after trimming
-----------------------------------------------------------------


--------option step:get rid of rRNA reads by sortmerna-----------
module load sortmerna
indexdb_rna -L 14 --ref ~/ufrc/nodulation/genome/rrna/peanut_rRNA_final.fasta,/ufrc/wang/luoziliang/nodulation/genome/rrna/peanut_rrna_index
# -L: seed length. default is 18. must be even number
#the software only take one input fasta file. need to merge paired reads first.
merge-paired-reads.sh 10_S22_1P.fastq 10_S22_2P.fastq 10_merged.fastq
#for i in *1P.fastq; do merge-paired-reads.sh ${i} ${i%_*}_2P.fastq ${i%%_*}_merged.fastq; done
sortmerna -m 4096 --log --num_alignments 1 --fastx --paired_out --ref ~/ufrc/nodulation/genome/rrna/peanut_rRNA_final.fasta,/ufrc/wang/luoziliang/nodulation/genome/rrna/peanut_rrna_index --reads 10_merged.fastq --aligned ./10_aligned_rrna --other ./10_non_rrna 
# -m: Mb ram to use
# -log: output overall statistics file
# --num-alignments: set 1 to output the first alignment passing E-value threshold (best choice if only filtering is needed-once have hit,considered as rRNA reads). defult:0 output all alignment.
# --fastx: output FASTA/FASTQ file for aligned or other reads.
# --aligned: path and base name of aligned read file
# --other: path and base name of rejected read file 
# --paired_in: put both pair to --aligned (rrna reads) file if one of the pair hit rRNA
# --paired-out: put both pair to --other (non-rrna reads) if only one of the pair hit rRNA
#separate merged unaligned files to paired files
unmerge-paired-reads.sh 10_non_rrna.fastq 10_nonrrna_1P.fq 10_nonrrna_2P.fq
#for i in *non_rrna.fastq; do unmerge-paired-reads.sh ${i} ${i%%_*}_nonrrna_1P.fq ${i%%_*}_nonrrna_2P.fq; done
-----------------------------------------------------------------


------------get rid of rRNA by bowtie alignment-------------------

#strategy 1. align to genome first with all reads
# for f in *1P.fq.gz; do echo ${f%_*}; hisat2 -p 4 -x /ufrc/wang/luoziliang/nodulation/genome/tif_gnm -1 ${f} -2 ${f%_*}_2P.fq.gz -U ${f%_*}_1U.fq.gz,${f%_*}_2U.fq.gz -S alignment/${f%R*}.sam; done
# cd alignment
# for f in *.sam; do echo ${f}; samtools sort -@ 4 -o ${f%.*}.bam ${f}; done

#strategy 2. align to the rRNA to get rid of rRNA. if didn't filter rRNA reads by sortmerna
bowtie2 -p 4 -x ./rrna_alignment2.2019/peanut_rrna_bt -1 15_S27_1P.fastq.gz -2 15_S27_2P.fastq.gz  -S 15_rrna_bowtie_end2end.sam

##get mapped and unmapped reads
samtools view -b -F 4 in.bam > mapped.bam
samtools view -b -f 4 in.bam > unmapped.bam
#for f in *.sam; do echo ${f}; samtools view -b -F 4 ${f}>${f%.*}_mapped.bam; done
#for f in *.sam; do echo ${f}; samtools view -b -f 4 ${f}>${f%.*}_unmapped.bam; done
#remove -b to output text .sam file

#get unmapped reads fastq file from sam/bam file
----------method1 for sam file by text definition-----
grep -v ^@ un_mapped.sam | awk 'NR%2==1 {print "@"$1"/1\n"$10"\n+\n"$11}' > unmapped1P.fq
grep -v ^@ un_mapped.sam | awk 'NR%2==0 {print "@"$1"/2\n"$10"\n+\n"$11}' > unmapped2P.fq
#e.g. grep -v ^@ 10_local_nonrrna.sam | awk 'NR%2==0 {print "@"$1"/2\n"$10"\n+\n"$11}' > 10_local_2P.fq
#	-v: to select non-matching lines(skip the headers)
#this method seperate paired read by "/". e.g. read1/1, read1/2
------------------------------------------------------
#--------method2 extract from bam file by software------
# samtools sort -n in.bam sorted.bam
#	-n: sort by read name
# bedtools bamtofastq [OPTIONS] -i <sorted.BAM> -fq <FASTQ1> -fq2 <FASTQ2>
#this method seperate paired read by "/" e.g. read1/1, read1/2
#by assign only -fq, it extract all unmapped reads
#by assign -fq -fq2, it extract paired reads. unpaired will be dumped!!!! DO NOT RECOMMEND THIS one
#for f in *.bam; do echo ${f}; samtools sort -n ${f} ${f%.*}_sorted.bam; done
#for f in *sorted.bam; do echo ${f}; bedtools bamtofastq -i ${f} -fq ${f%.*}_1P.fq -fq2 ${f%.*}_2P.fq; done
# ------------------------------------------------------

----------------small RNA alignment---------------------
# 1. alignment by miRge
# module load mirge cutadapt/1.8.1
# #build database, it needs to be written to software path. Already built for peanut
# #miRge-build.pl --species peanut --mirna ahy_mature.fasta --hairpin ahy_premature.fasta --other peanut_tRNA_RNAcentral.fasta --mrna arahy.Tifrunner.gnm1.ann1.CCJH.cds.fna
# miRge.pl --adapter none --species peanut --diff-isomirs --SampleFiles 10_pair1_short_reads.fq,10_pair2_short_reads.fq
# # --diff-isomirs: report isoform of mirna, default off. If the percentage of isomir is low, it's probably alignment mistake/artifact

2. alignment by mirDeep2
module load bowtie mirdeep 

#2.1 index genome
bowtie-build arahy.Tifrunner.gnm1.KYV3.genome_main.fna tif_gnm_bt
#merge paired files into one fastq file
cat 10_pair1_short_reads.fq 10_pair2_short_reads.fq>10_merged_short_reads.fq
#or use script from sortmerna module to merge
#for f in *pair1_short_reads.fq; do merge-paired-reads.sh ${f} ${f%%_*}_pair2_short_reads.fq ${f%%_*}_merged_short_reads.fq; done
#remove space in read ID
python3 adjust_fasta_id.py
#transfer fastq file to fasta file 
module load seqtk
for f in *_merged_short_reads.fq; do seqtk seq -a ${f} > ${f%.*}.fasta; done
#create config.txt file to designate reads files to respective samples
#eg. s10 10_merged_short_reads_rev.fasta
#	 s11 11_merged_short_reads_rev.fasta
#2.2 align to genome
mapper.pl ../10_merged_short_reads_rev.fasta -c -j -m -p ~/ufrc/nodulation/genome/tif_gnm_bt -s 10_short_clp.fa -t 10_short_clp_to_genome.arf -v
#for f in *merged_short_reads.fasta;do mapper.pl ${f} -c -j -m -p ~/ufrc/nodulation/genome/tif_gnm_bt -s mirdeep/${f%merged*}short_clp.fa -t mirdeep/${f%merged*}short_clp_to_genome.arf -v; done
#mapper.pl config.txt -d -j -m -p ~/ufrc/nodulation/genome/tif_gnm_bt -s reads_short_clp.fa -t reads_short_clp_to_genome.arf -v
# output: bowtie.log, 2 collapsed files(*clp*.arf:mapping file, *clp*.fa) 
# -c: designate that input file is a FASTA
# -d: designate that input file is config file
# -j: remove non-canonical letter (not atucgn or ATUCGN)
# -l: filter option, assign read length threshold that should be dropped
# -m: collapses the reads
# -p: maps the processed reads against the previously indexed genome 
# -s: designates the name of the output file of processed reads
# -t: designates the name of the output file of the genome mappings
# -r: allow number of multi alignment to genome. default 5

#2.3 fast quantitation of reads mapping to known miRBase precursors.
#need to load python2 as default
quantifier.pl -p ahy_premature.fasta -m ahy_mature.fasta -r 10_short_clp.fa
#for f in *short_clp.fa;do quantifier.pl -p ~/ufrc/nodulation/genome/mirna/ahy_premature.fasta -m ~/ufrc/nodulation/genome/mirna/ahy_mature.fasta -r ${f}; done
# output: expression.html; expression.csv; pdf
# -W: read counts are weighed by their number of mappings. e.g. A read maps twice so each position gets 0.5 added to its read profile
# -g: number of allowed mismatches when mapping reads to precursors, default 1
# -e: number of nucleotides upstream of the mature sequence to consider, default 2
# -f: number of nucleotides downstream of the mature sequence to consider, default 5
# -y: [time], add time/tag to result
#2.4 identification of known and novel miRNAs in the deep sequencing data:
miRDeep2.pl 10_short_clp.fa ~/ufrc/nodulation/genome/arahy.Tifrunner.gnm1.KYV3.genome_main.fna 10_short_clp_to_genome.arf ahy_mature.fasta Ath_legume_miRBase.fasta ahy_premature.fasta 2> 10_report.log
# for f in *short_clp.fa; do miRDeep2.pl ${f} ~/ufrc/nodulation/genome/arahy.Tifrunner.gnm1.KYV3.genome_main.fna ${f%clp*}clp_to_genome.arf ~/ufrc/nodulation/genome/mirna/ahy_mature.fasta ~/ufrc/nodulation/genome/mirna/Ath_legume_miRBase.fasta ~/ufrc/nodulation/genome/mirna/ahy_premature.fasta 2> ${f%short*}_report.log; done
# output: result.csv; result.html
# -z: sample_name. add tag to the time stamp. recommend to add sample name
--------------------------------------------------------


----------------align to genome for long reads-------------------------
hisat2-build arahy.Tifrunner.gnm1.KYV3.genome_main.fna tif_gnm
hisat2 -p 8 -x /home/luoziliang/ufrc/nodulation/genome/tif_gnm --known-splicesite-infile /home/luoziliang/ufrc/nodulation/rnaseq/genome/splice_sites.gtf -1 10_pair1_long_reads.fq -2 10_pair2_long_reads.fq -S 10_long_gnm.sam
for f in *pair1_long_reads.fq; do echo ${f} ${f%pair*}pair2_long_reads.fq; hisat2 -p 4 -x /home/luoziliang/ufrc/nodulation/genome/tif_gnm -1 ${f} -2 ${f%pair*}pair2_long_reads.fq -S ${f%%_*}_long_gnm.sam; samtools sort -@ 8 -o ${f%%_*}_long_gnm.bam ${f%%_*}_long_gnm.sam; echo finished\t${f%%_*}; done
#for f in *.sam; do samtools sort -@ 8 -o ${f%.*}.bam ${f}; echo finished\t${f}; done

#optional: filter out multiple aligned reads
samtools view -h myBAM.bam | grep -P "(NH:i:1\b|NH:i:2\b|^@)" | samtools view -Sb - > myBAM_filtered.bam
#select reads aligned to less than 2 alignments and headers started with "@"

#assemble transcripts for each sample
for f in *.bam; do echo ${f}; stringtie -p 8 -f 0.3 -j 4 -c 7.0 -G /home/luoziliang/ufrc/nodulation/genome/arahy.Tifrunner.gnm1.ann1.CCJH.gene_models_main.gff3 -o ${f%.*}.gtf -l ${f%.*} ${f}; done
# to get uniquely aligned reads, set -M to 0 or 0.1
# -M <0.0-1.0>	Sets the maximum fraction of muliple-location-mapped reads that are allowed to be present at a given locus. Default: 0.95.
# -e only estimate and output the assembled transcripts matching the reference transcripts given with the -G option
# -c Sets the minimum read coverage allowed for the predicted transcripts.Default: 2.5
# -f Sets the minimum isoform abundance of the predicted transcripts as a fraction of the most abundant transcript assembled at a given locus. Lower abundance transcripts are often artifacts of incompletely spliced precursors of processed transcripts
# -j There should be at least this many spliced reads that align across a junction (i.e. junction coverage)

#merge assembled transcripts from all sample. need to make mergelist.txt file by yourself first.
stringtie --merge -p 4 -T 1 -G /home/luoziliang/ufrc/nodulation/genome/arahy.Tifrunner.gnm1.ann1.CCJH.gene_models_main.gff3 -o stringtie_merged.gtf ./mergelist.txt
# -m <min_len>	minimum input transcript length to include in the merge (default: 50)
# -c <min_cov>	minimum input transcript coverage to include in the merge (default: 0)
# -F <min_fpkm>	minimum input transcript FPKM to include in the merge (default: 0)
# -T <min_tpm>	minimum input transcript TPM to include in the merge (default: 0)
# -f <min_iso>	minimum isoform fraction (default: 0.01)
# -g <gap_len>     gap between transcripts to merge together (default: 250)
#check how many transcript assembled
awk '$3=="transcript"' stringtie_merged.gtf | wc -l

#compare assembled transcripts to ref genes.
module load gffread/0.9.8c
gffcompare -r /home/luoziliang/ufrc/nodulation/genome/arahy.Tifrunner.gnm1.ann1.CCJH.gene_models_main.gff3 -G -o merged stringtie_merged.gtf
# â€“G: tells gffcompare to compare all transcripts in the input

#add ref gene ID to assembled transcript ID if the ref gene overlapped with the transcript
python3 mstrg_gtf_file_prep.py stringtie_merged.gtf > stringtie_merged_IDmodified.gtf

#Estimate transcript abundances and create table counts for Ballgown
for f in *.bam; do echo ${f}; stringtie -e -B -p 8 -G stringtie_merged_IDmodified.gtf -o ./ballgown/${f%.*}/${f%.*}.gtf ${f}; done
# -e estimate
# -B output of Ballgown input table files (*.ctab) containing coverage data for the reference transcripts given with the -G option.

#extract reads count for DEG analysis
python prepDE.py
-------------------------------------------------------------------


#--------------------DEG analysis by DEseq2 in R-------------------------
module load R
R
library("DESeq2")
#Load gene(/transcript) count matrix and labels
countData <- as.matrix(read.csv("gene_count_matrix.csv", row.names="gene_id"))
#use "transcript_count_matrix.csv" to calculate if need to include noncoding RNA or gene isoforms
#countData <- as.matrix(read.csv("transcript_count_matrix.csv", row.names="transcript_id"))
colData <- read.csv('samples.csv', row.names=1)
#Check all sample IDs in colData are also in CountData and match their orders
all(rownames(colData) %in% colnames(countData))
countData <- countData[, rownames(colData)]
all(rownames(colData) == colnames(countData))
#output should be true
dds <- DESeqDataSetFromMatrix(countData = countData,colData = colData, design=~group) 
#to eliminate bach effect,add batch/replicate to the design: design=~batch+group
#group is the colname of your treatment
#Run the default analysis for DESeq2 and generate results table
dds <- DESeq(dds)
res <- results(dds)
#the result() function have default setting to remove low expression genes (independentFiltering = TRUE)
##############################
#if have multiple comparison, subset each comparison:
#colData1 = subset(colData, treatment%in%c("10dpg","4dpi"))
#colData2 = subset(colData, treatment%in%c("10dpg","8dpi"))
#colData6 = subset(colData, genotype%in%"E7")
#countData1 <- countData[, rownames(colData)]
#change the loop range according to the number of comparison
# for (i in 1:8){     #i is the number of comparison you have
#     print(paste("started: dds", i, sep = ""))
#     assign(paste("dds", i, sep = ""), DESeqDataSetFromMatrix(countData = get(paste("countData", i, sep="")),colData = get(paste("colData", i, sep="")), design=~treatment)); 
#     assign(paste("dds", i, sep = ""),DESeq(get(paste("dds",i,sep=""))));
#     assign(paste("res", i, sep = ""),results(get(paste("dds",i,sep=""))))
#     print(paste("finished res", i, sep = ""))
# }
###############################
write.csv(res,"deseq2_*_result.csv")


#do the PCA plot
pdf("PCA.pdf")
#estimate dispersion trend and apply a variance stabilizing transformation
vsdata <- vst(dds, blind=FALSE)
plotPCA(vsdata, intgroup="treatment")
dev.off()

#get the basemean for each sample 
baseMeanPerLvl <- sapply( levels(dds$treatment), function(lvl) rowMeans( counts(dds,normalized=TRUE)[,dds$treatment == lvl] ) )
write.csv(baseMeanPerLvl,'basemean_per_sample.csv')

##vocalno plot
#DEG result file must end with 'result.csv'
Rscript vocalno_plot.r
-----------------------------------------------------------------------

--------------identify orthologs in assembled transcripts----------------
module load samtools gffread transdecoder
##create index for matching
samtools faidx /ufrc/wang/luoziliang/nodulation/genome/arahy.Tifrunner.gnm1.KYV3.genome_main.fna
#extract sequence from assembled gtf file. output assembled transcripts sequence as transcripts.fa
gffread -w transcripts.fa -g /ufrc/wang/luoziliang/nodulation/genome/arahy.Tifrunner.gnm1.KYV3.genome_main.fna ../stringtie_merged_IDmodified.gtf
#or you can use transdecoder module to extract sequence:
#gtf_genome_to_cdna_fasta.pl transcripts.gtf test.genome.fasta > transcripts.fasta 
##convert gtf file to gff3 file 
gtf_to_alignment_gff3.pl stringtie_merged_pe.gtf >stringtie_merged_pe.gff3
##predict the candidate ORF and extract the peptide sequence
TransDecoder.LongOrfs -t transcripts.fa -t transcripts.fa
TransDecoder.Predict -t transcripts.fa
--------------------------------------------------------------------------

-----------------------------lncRNA analysis------------------------------
#extract non-gene-model/possible lncRNA transcripts
awk '{if ($3=="u" || $3=="x" || $3=="i" || $3=="j" || $3=="o" || $3=="class_code"){print $0}}' merged.stringtie_merged.gtf.tmap > non_gene_model
#'
#extract assembled transcript sequence
module load samtools
module load gffread
#indexing
samtools faidx /ufrc/wang/luoziliang/nodulation/genome/arahy.Tifrunner.gnm1.KYV3.genome_main.fna
#extraction
gffread -w transcripts.fa -g /ufrc/wang/luoziliang/nodulation/genome/arahy.Tifrunner.gnm1.KYV3.genome_main.fna stringtie_merged_IDmodified.gtf

##get non_gene_model candidate list
awk '{print $5}' non_gene_model > non_gene_model_list
##extract noncoding sequence       
module load kent
faSomeRecords transcripts.fa non_gene_model_list non_gene_model_stringtie.fasta
##use fasta_handle.py to remove redundant sequence and filter <200nt
python3 fasta_handle.py -i non_gene_model_stringtie.fasta -o non_gene_model_stringtie200.fasta -c -l 200

#(1)Predict noncoding by CPC2.
module load gcc cpat
python ufrc/CPC2-beta/bin/CPC2.py -i non_gene_model_E7_2h_stringtie200.fasta -o CPC_E7_2h_result.txt
#(2)Predict noncoding by CPAT.
#make your own hexamer for prediction
make_hexamer_tab.py -c arahy.Tifrunner.gnm1.ann1.CCJH.cds.fna -n plant_uniq_lnc.fasta > Peanut_Hexamer.tsv
#build logit model
make_logitModel.py -x Peanut_Hexamer.tsv -c arahy.Tifrunner.gnm1.ann1.CCJH.cds.fna -n plant_uniq_lnc.fasta -o Peanut
#testing the model by using known gene and lncrna
cat Araport11_genes.201606.cds.fasta plant_uniq_lnc.fasta >test.fasta
cpat.py -g test.fasta -d Peanut.logit.RData -x Peanut_Hexamer.tsv -o test
#change the colname of test result: ID	mRNA	ORF	Fickett	Hexamer	Label
#the score of known should be change to 1, noncoding to 0. only 2 class allowed
Rscript 10Fold_CrossValidation.r

#predict the non-gene_model transcripts
cpat.py -g non_gene_model_stringtie200.fasta -d Peanut.logit.RData -x Peanut_Hexamer.tsv -o peanut_cpat_result

------------------------------------------------------------------------------



---------------------------DEXSeq analysis------------------------------------
#install/load HTSeq software. compatible with python2 and python3(<3.7) on HPC
#locate the python script for the data preparation
pythonScriptsDir = system.file( "python_scripts", package="DEXSeq" )
list.files(pythonScriptsDir)
## [1] "dexseq_count.py"              "dexseq_prepare_annotation.py"
system.file( "python_scripts", package="DEXSeq", mustWork=TRUE )
## [1] "/private/tmp/RtmphVyXtk/Rinstce4770deee3f/DEXSeq/python_scripts"
#require alignment .sam file and annotation of gtf file
#transform gff3 file to gtf file
gffread -T arahy.Tifrunner.gnm1.ann1.CCJH.gene_models_main.gff3 -o arahy.gene.gtf 

#put .gtf file, two python scripts and .sam alignment files in same directory.
#prepare annotation of the exons
python dexseq_count.py ahy.DEXSeq.gtf
#count the reads number
python dexseq_count.py -s no ahy.DEXSeq.gtf SRR5043459.sam SRR5043459.txt
# -p yes: if use paired end reads alignment, but all reads must have a pair.
# -s no:  if reads are not from strand specific library construction
# -f bam: can use BAM alignment files only if pysam is installed on python.
#for multiple file jobs, use following:
# for i in SRR504348*.sam; do echo ${i}; python dexseq_count.py -s no ahy.DEXSeq.gtf ${i} ${i%.*}.txt; done

#read data to R
#show the count files and annotation file in your working directory
countFiles = list.files(path = getwd(), pattern=".txt", full.names=FALSE)
basename(countFiles)      
flattenedFile = list.files(path = getwd(), pattern="gtf", full.names=FALSE)
basename(flattenedFile)
#load library table
sampleTable = read.csv('pe_samples.csv')
#'pe_samples.csv' example format:
# 				condition		genotype
# SRR5043458	E4_treat_1     E4
# SRR5043459	E4_treat_2     E4
# SRR5043460	E4_control_1   E4

#subsetdata for multiple comparison
sampleTable1 = subset(sampleTable,genotype%in%"E4")

dxd = DEXSeqDataSetFromHTSeq(
   countFiles,
   sampleData=sampleTable,
   design= ~ sample + exon + condition:exon,
   flattenedfile=flattenedFile )

#View the data
colData(dxd)
#View the count
head( counts(dxd), 5 )
#there will be double number of columns representing number of reads mapping to out exonic region and ssum of the counts mapping to the rest of the exons
#check it using:
split( seq_len(ncol(dxd)), colData(dxd)$exon )
#or view the samples with count aligning to exonic regions(without showing the sum of counts from the rest of the exons from the same gene)
head( featureCounts(dxd), 5 )
#can use multiple core for faster computation
library(BiocParallel)
BPPARAM = MultiCoreParam(4)
#Normalisation
dxd = estimateSizeFactors( dxd )
#Dispersion estimation
dxd = estimateDispersions( dxd, BPPARAM=BPPARAM)
plotDispEsts( dxd )
#tests for each exon in each gene
dxd = testForDEU( dxd, BPPARAM=BPPARAM)
#estimate relative exon usage fold changes,calculated based on the coefficients of a GLM fit that uses the formula
dxd = estimateExonFoldChanges(dxd, BPPARAM=BPPARAM)
#get the result from previous steps
dxr1 = DEXSeqResults( dxd )
write.csv(dxr1,'dxr_e4.csv')
##summary of the result
#number of exons are significantly changed
table ( dxr1$padj < 0.05 )
#number of genes inovolved in exon changes
table ( tapply( dxr1$padj < 0.1, dxr1$groupID, any ) )
#plot the differential exon usage
#Mean expression versus log2fold change plot. Significant hits at an FDR=0.1 are coloured in r
plotMA( dxr1, alpha=0.05,)
#plot specific gene sturcture with differential exon usage
plotDEXSeq( dxr1, "arahy.Tifrunner.gnm1.ann1.1FS32P", legend=TRUE,cex.axis=1.2, cex=1.3, lwd=2 )
#to generate overall report of all study
DEXSeqHTML( dxr1, FDR=0.05,file="testForDEU_E4.html", color=c("#FF000080", "#0000FF80") )


####alternatively, use one step for the whole analysis
# dxr = DEXSeq(dxd)
# class(dxr)


----------------------------------------------------------------------------

------------------------JunctionSeq (pending, alignment must be paired reads only or single end reads only)-----------------------------------------
#splice-junction and gene counts must be generated via QoRTs
#for paired reads, need to sort by name first
for i in *.bam; do echo ${i}; samtools sort -n -o ${i} ${i};done

module load java
java -jar hartleys-QoRTs-099881f/QoRTs.jar QC --nameSorted --runFunctions writeKnownSplices,writeNovelSplices,writeSpliceExon ./SRR5043458.bam arahy.gene.gtf rawCts/SRR5043458
#for i in SRR50434*.bam; do echo	started\t${i}; java -jar hartleys-QoRTs-099881f/QoRTs.jar QC --runFunctions writeKnownSplices,writeNovelSplices,writeSpliceExon ${i} arahy.gene.gtf rawCts/${i%.*}; done

-----------------------------------------------------------------------------


-----------------------Alternative splicing by MATS----------------------------
module load mats
rmats.py --b1 E7_treated.txt --b2 E7_control.txt --gtf /ufrc/wang/luoziliang/nodulation/genome/DEXSeq/arahy.gene.gtf --od E7_test -t paired --readLength 140
##E7_treated.txt example:
# SRR5043464.bam,SRR5043466.bam,SRR5043468.bam
# or directly input bam files after --b2

#Plot the result using rmats2sashimiplot
module load rmats2sashimiplot
# rmats2sashimiplot --b1 SRR5043463.bam,SRR5043465.bam,SRR5043493.bam --b2 SRR5043467.bam,SRR5043472.bam,SRR5043483.bam -c chrarahy.Tifrunner.gnm1.Arahy.10:-:102916:106308:/ufrc/wang/luoziliang/nodulation/genome/DEXSeq/arahy.gene.gtf --l1 E4_treated --l2 E4_control -o E4_test_coordinate_output 
rmats2sashimiplot --b1 SRR5043463.bam,SRR5043465.bam,SRR5043493.bam --b2 SRR5043467.bam,SRR5043472.bam,SRR5043483.bam -t SE -e ./E4_test/SE.MATS.JC.txt --l1 E4_treated --l2 E4_control --exon_s 1 --intron_s 5 -o E4_test_events_output --group-info grouping.gf
# -t eventType
# -e eventsFile
# grouping file grouping_E4.gf:
# group1name: 1-3
# group2name: 4-6









