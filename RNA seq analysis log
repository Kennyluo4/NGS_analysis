###RNA seq re-analysis log###
#If there are too many samples and files are too big, it's recommended to run files separately by specifying different file pattern. e.g. SRR504349*_1.fastq for 10 files/SRR504348*_1.fastq for another 10 files.
#python scripts required are available at: https://github.com/Kennyluo4/my_analysis and sharedrive ./Ziliang/4.scripts
#folder structure  
#|rnaseq
#	|genome     (store genome, annotation files)
#		|index  (store created index)
#	|data       (the working directory, store reads files)
#		|FastQC
#		|FastQC_after_trim
#		|aling_se
#		|align_pe
 
1.	Data download from NCBI SRA/GEO database.
1)	download the accession list from database
  	Dasgupta (india):https://www.ncbi.nlm.nih.gov/sra?term=SRP107173
	Ze:https://www.ncbi.nlm.nih.gov/sra/SRP093688
2)	using sra toolkit for downloading and transforming data (installed on hpg)
module load sra
vdb-config -i                   
##change software config, downloading location. Avoid storage violation 
prefetch --option-file SraAccList.txt
##SraAccList.txt is the list of target sequencing files
for f in ./*.sra; do fastq-dump --skip-technical  --readids --dumpbase --split-files $f; done
##transform sra format from website to fastq format


2.	Quality control using fastQC (before trimming)
module load  fastqc
module load python3
mkdir FastQC
cp get_fastqc.py ./FastQC   ##script for get the QC summary for all files
fastqc *.fastq -o FastQC
python3 ./FastQC/get_fastqc.py
module load gcc
#requirement for multiqc
module load multiqc
cd FastQC_after_trim
multiqc . 


3.	Trimming using Trimmomatic
module load trimmomatic
##pair end reads trimming
for file in SRR504349*_1.fastq; do echo ${file%_*}; trimmomatic PE ${file%_*}_1.fastq ${file%_*}_2.fastq -baseout ${file%_*}.fq.gz ILLUMINACLIP:/ufrc/wang/luoziliang/nodulation/rnaseq/adapters.fasta:2:30:20:8:true LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:50; done
## {base%.*} is to delete the every str(*) rightside of "." in base
python3 get_trim_report.py          ##script for summarize the trimming report for all file, output .csv file
##single end reads
#for f in SRR557230*.fastq; do echo ${f%_*}; trimmomatic SE $f ${f%.*}.fq.gz ILLUMINACLIP:/ufrc/wang/luoziliang/nodulation/rnaseq/adapters.fasta:2:30:20 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:50; done

##parameters for the trimming
	•	LLUMINACLIP: Cut adapter and other illumina-specific sequences from the read.
		<fastaWithAdaptersEtc>:<seed mismatches>:<palindrome clip threshold>:<simple clip threshold>
		2 parameters can be added:<minAdapterLength>:<keepBothReads> default is :8:false, it will remove palindrome reads. set as true will rescue more reads
	•	SLIDINGWINDOW: Perform a sliding window trimming, cutting once the average quality within the window falls below a threshold.
	•	LEADING: Cut bases off the start of a read, if below a threshold quality
	•	TRAILING: Cut bases off the end of a read, if below a threshold quality
	•	HEADCROP: Cut the specified number of bases from the start of the read
	•	MINLEN: Drop the read if it is below a specified length
	
	
4. QC again to check the quality
mkdir FastQC_after_trim
fastqc SRR*.fq.gz -o FastQC_after_trim
cp get_fastqc.py ./FastQC_after_trim
python3 ./FastQC_after_trim/get_fastqc.py        
# using multiqc to generate report
module load gcc
#requirement for multiqc
module load multiqc
cd FastQC_after_trim
multiqc . 


5. alignment to reference genome  (hisat2)
5.1 indexing
# files required: reference genome.fasta; gene annotation.gff    (or gtf)
#if the gene annotation is gff3 format, use gffread to transform to gtf format, because hisat builtin script only support gtf format
module load hisat
module load gffread
gffread arahy.Tifrunner.gnm1.ann1.CCJH.gene_models_main.gff3 -T -o genes.gtf
#using builtin scripts to get exon and splice site information for the alignment
hisat2_extract_exons.py genes.gtf >exon.gtf	
hisat2_extract_splice_sites.py genes.gtf >splice_sites.gtf
#It is highly recommended to use the reference annotation information when mapping the reads, which can be either embedded in the genome index (built with the -ss and --exon options, see HISAT2 manual), or provided separately at alignment step by --known-splicesite-infile
#indexing
hisat2-build --ss splice_sites.gtf --exon exons.gtf arahy.Tifrunner.gnm1.KYV3.genome_main.fna tif_gnm
#most likely you cannot built index with annotation because of memory limitation. If you use --snp, --ss, and/or --exon, hisat2-build will need about 200GB RAM for the human genome size as index building involves a graph construction. 
#Otherwise, you will be able to build an index on your desktop with 8GB RAM.

5.2 alignment
#from fasta file to sequence alignment file(SAM) and transform to BAM file
module load samtools
module load hisat2
for f in SRR504349*1P.fq.gz; do echo ${f%.*}; hisat2 -p 4 -x /home/luoziliang/ufrc/nodulation/rnaseq/genome/index/tif_gnm --known-splicesite-infile /home/luoziliang/ufrc/nodulation/rnaseq/genome/splice_sites.gtf -1 ${f} -2 ${f%_*}_2P.fq.gz -U ${f%_*}_1U.fq.gz,${f%_*}_2U.fq.gz -S ./align_pe/${f%_*}.sam | samtools sort -@ 8 -o ${f%_*}.bam ${f%_*}.sam; done
#for f in SRR55723*.fq.gz; do echo ${f%.*}; hisat2 -p 8 -x /home/luoziliang/ufrc/nodulation/rnaseq/genome/index/tif_gnm --known-splicesite-infile /home/luoziliang/ufrc/nodulation/rnaseq/genome/splice_sites.gtf -U ${f} -S ./aling_se/${f%_*}.sam | samtools sort -@ 8 -o ./aling_se/${f%_*}.bam ./aling_se/${f%_*}.sam; done
#hisat2 parameters:
# -p: number of threads/cores to use for the software(default: 1)
# -x: path of reference genome index
# -1: paired file 1       -2: paired file 2      -U: unpaired files    (separated by comma)
# -S: output sam file
# --known-splicesite-infile: path of gene annotation file(use the extracted splice_sites.gtf)
#samtools parameters:
# -@: number of cores used
# -o: samtools output assigned by user

#extract the alignment report
python get_align_stat.py align_se_log_27669981.out


6.assembly  (stringtie)
#input: aligned transcripts (BAM file)
#output: assembled transcript description (GTF file)
#assemble the transcript with stingtie for each sample
module load stringtie
for f in *.bam; do echo ${f}; stringtie -p 8 -G /home/luoziliang/ufrc/nodulation/rnaseq/genome/genes.gtf -o ${f%.*}.gtf -l ${f%.*} ${f}; done
#parameters:
# -G: the reference annotation file
# -p: number of threads/cores to use for the software(default: 1)
# -l: Sets <label> as the prefix for the name of the output transcripts (default: STRG)
# --merge: Transcript merge mode. In the merge mode, StringTie takes as input a list of GTF/GFF files and merges/assembles these transcripts into a non-redundant set of transcripts. 

#merge assembled transcripts from each sample to a non-redundant file to represent all transcripts obtained from the expriment.
stringtie --merge -p 8 -G /home/luoziliang/ufrc/nodulation/rnaseq/genome/genes.gtf -o stringtie_merged_se.gtf /home/luoziliang/ufrc/nodulation/rnaseq/data/sra/aling_se/mergelist.txt
# -m <min_len>	minimum input transcript length to include in the merge (default: 50)
# -c <min_cov>	minimum input transcript coverage to include in the merge (default: 0)
# -F <min_fpkm>	minimum input transcript FPKM to include in the merge (default: 0)
# -T <min_tpm>	minimum input transcript TPM to include in the merge (default: 0)
# -f <min_iso>	minimum isoform fraction (default: 0.01)
#after the merge, the output gtf file doesn't include fpkm information
#mergelist.txt is file of each samples .gtf file, one file name in one line
#  e.g. sample1.gtf
		  sample2.gtf

##to compare assembled transcripts with reference annotations/gene models
module load gffread/0.9.8c
gffcompare -r /home/luoziliang/ufrc/nodulation/rnaseq/genome/genes.gtf -G -o merged_se stringtie_merged_se.gtf
# –G: tells gffcompare to compare all transcripts in the input
#output 5 files: 
	
	merged_se.stats,           ##Overall summary statistics
	merged_se.loci, 
	merged_se.stringtie_merged_se.gtf.refmap,     ##Transfrags matching to each reference transcript
	merged_se.stringtie_merged_se.gtf.tmap,       ##Best reference transcript for each transfrag. Important file, including transcripts, refgeneand fpkm information
	merged_se.annotated.gtf                       ##annotation for input transcripts
	merged_se.tracking,                           ##information similar to *.tmap

------------------------------------------------
#recommended step for gene level downstream analysis
#to append the refgene ID to StringTie assinged ID for genes that include a gene model as one of isoforms.
python mstrg_prep.py stringtie_merged_se.gtf > stringtie_merged_se_IDmodified.gtf
------------------------------------------------

#Estimate transcript abundances and create table counts for Ballgown
for f in *.bam; do echo ${f}; stringtie -e -B -p 8 -G stringtie_merged_se.gtf -o ./ballgown/${f%.*}/${f%.*}.gtf ${f}; done
# -e estimate
# -B output of Ballgown input table files (*.ctab) containing coverage data for the reference transcripts given with the -G option.
# -M Sets the maximum fraction of muliple-location-mapped reads that are allowed to be present at a given locus. Default: 0.95
--------------------------identify orthologs in assembled transcripts------------------------
module load samtools gffread transdecoder
##create index for matching
samtools faidx /ufrc/wang/luoziliang/nodulation/rnaseq/genome/arahy.Tifrunner.gnm1.KYV3.genome_main.fna
#extract sequence from assembled gtf file. output assembled transcripts sequence as transcripts.fa
gffread -w transcripts.fa -g /ufrc/wang/luoziliang/nodulation/rnaseq/genome/arahy.Tifrunner.gnm1.KYV3.genome_main.fna stringtie_merged_se.gtf
#or you can use transdecoder module to extract sequence:
#gtf_genome_to_cdna_fasta.pl transcripts.gtf test.genome.fasta > transcripts.fasta 
##convert gtf file to gff3 file 
gtf_to_alignment_gff3.pl stringtie_merged_pe.gtf >stringtie_merged_pe.gff3
##predict the candidate ORF and extract the peptide sequence
TransDecoder.LongOrfs -t transcripts.fa -t transcripts.fa
TransDecoder.Predict -t transcripts.fa
----------------------------------------------------------------------------------------------


7. DEG
##by ballgown
R
#--------------------processing in R----------------------------------------------------------
library(ballgown)
library(RSkittleBrewer)
library(genefilter)
library(dplyr)
library(devtools)
pheno_data = read.csv("se_samples.csv", header=T)
pheno_data = pheno_data[order(pheno_data$ID),]  #necessary for the algarithm
bg_data = ballgown(dataDir= "ballgown", samplePattern= "SRR", pData=pheno_data)
bg_filt = subset(bg_data,"rowVars(texpr(bg_data)) >1",genomesubset=TRUE) #Remove low-abundance.remove all transcripts with a variance across samples less than one
#Load gene names for lookup later
bg_table = texpr(bg_filt, 'all')
bg_gene_names = unique(bg_table[, 9:10])
#Pull the gene_expression data frame from the ballgown object
gene_expression = as.data.frame(gexpr(bg_filt))
#Load the transcript to gene index from the ballgown object
transcript_gene_table = indexes(bg_data)$t2g
head(transcript_gene_table)
length(row.names(transcript_gene_table)) #Transcript count
length(unique(transcript_gene_table[,"g_id"])) #Unique Gene count
#--------------------------------------------------------------------------------------------------


##By DEseq2
#using prepDE.py script to collect count information for DEseq2 input
python prepDE.py
#prepDE.py either accepts a .txt (sample_lst.txt) file listing the sample IDs and the GTFs' paths or by default expects a ballgown directory produced by StringTie run with the -B option(created folder for ballgown)
#gene_count_matrix.csv is the geneID assigned by stringtie
#if more isoforms are assembled for a reference gene, StringTie gives the gene a new ID(MSTRG.*) instead of ref gene ID
#you can either append ref ID using: python mstrg_prep.py stringtie_merged_se.gtf > stringtie_merged_se.gtf after merged all transcript (last of step 6)
#or using: python extract_assembled_id_from_stringtie.py stringtie_merged_se.gtf to extract the corresponding IDs
#after prepDE.py generated count_matrix data, replace the StringTie ID with corresponding ref gene ID in count_matrix.csv before DESeq2.

module load R
R
#--------------------processing in R-------------------------------------------------------------------------
library("DESeq2")
#Load gene(/transcript) count matrix and labels
countData <- as.matrix(read.csv("gene_count_matrix.csv", row.names="gene_id"))
#use "transcript_count_matrix.csv" to calculate if need to include noncoding RNA or gene isoforms
#countData <- as.matrix(read.csv("transcript_count_matrix.csv", row.names="transcript_id"))
colData <- read.csv('se_samples.csv', row.names=1)
#Check all sample IDs in colData are also in CountData and match their orders
all(rownames(colData) %in% colnames(countData))
countData <- countData[, rownames(colData)]
all(rownames(colData) == colnames(countData))
#output should be true
dds <- DESeqDataSetFromMatrix(countData = countData,colData = colData, design=~group) 
#group is the colname of your treatment
#Run the default analysis for DESeq2 and generate results table
dds <- DESeq(dds)
res <- results(dds)
#the result() function have default setting to remove low expression genes (independentFiltering = TRUE)
##############################
#if have multiple comparison, subset each comparison:
#colData1 = subset(colData, treatment%in%c("10dpg","4dpi"))
#colData2 = subset(colData, treatment%in%c("10dpg","8dpi"))
#colData6 = subset(colData, genotype%in%"E7")
#countData1 <- countData[, rownames(colData)]
#change the loop range according to the number of comparison
#for (i in 1:6){
# 	assign(paste("dds", i, sep = ""), DESeqDataSetFromMatrix(countData = get(paste("countData", i, sep="")),colData = get(paste("colData", i, sep="")), design=~treatment)); 
# 	assign(paste("dds", i, sep = ""),DESeq(get(paste("dds",i,sep=""))));
# 	assign(paste("res", i, sep = ""),results(get(paste("dds",i,sep=""))))
# }
###############################
write.csv(res,"deseq2_*_result.csv")

#do the PCA plot
pdf("PCA.pdf")
vsdata <- vst(dds, blind=FALSE)
plotPCA(vsdata, intgroup="group")
dev.off()

##vocalno plot
#DEG result file must end with 'result.csv'
library(ggplot2)
files <-list.files(pattern="result.csv")
for (i in 1:length(files)){
	data <- read.csv(files[i],header = T, row.names = 1)
	picname <- paste(gsub(".csv","",files[i]),".pdf")
	P <- data[,6]
	#FC <- data[,4]
	logFC = data[,2]
	df <- data.frame(P,logFC)
	df$threshold <- as.factor(abs(logFC)>1 & P <0.05)
	ggplot(df, aes(x=logFC, y = -log10(P), colour=threshold)) + geom_point(alpha=0.7, size=1) + xlab("log2 fold change") + ylab("-log10 p-value")+theme(legend.position='none')+ geom_vline(xintercept = c(-1,1)) + geom_hline(yintercept = -log10(0.05))
	ggsave(picname)
}
#-------------------------------------------------------------------------------------------------------

#code for volcano plot can be saved as R script volcano_plot.r and run in command line:
Rscript volcano_plot.r





####lncrna analysis
#extract unperfect match transcripts as noncoding rna candidate	based on transcript classification
#extract from all assembles transcripts (stringtie_merged_se.gtf)
awk '{if ($3=="u" || $3=="x" || $3=="i" || $3=="j" || $3=="o" || $3=="class_code"){print $0}}' merged_se.stringtie_merged_se.gtf.tmap > non_gene_model
#classification of transcripts
		=   Complete match of intron chain
		j   Potentially novel isoform (fragment): multi exon with at least one splice junction shared with a reference transcript
		i   A transfrag falling entirely within a reference intron
		o   Generic exonic overlap with a reference transcript(other same strand overlap with reference exons)
		u   Unknown, intergenic transcript
		x   Exonic overlap with reference on the opposite strand

##get all sequence from merged stringtie file.
##input: stringtie_merged_se.gtf; genome.fna
module load samtools gffread
##create index for matching
samtools faidx /ufrc/wang/luoziliang/nodulation/rnaseq/genome/arahy.Tifrunner.gnm1.KYV3.genome_main.fna
#extract sequence from assembled gtf file. output assembled transcripts sequence as transcripts.fa
gffread -w transcripts.fa -g /ufrc/wang/luoziliang/nodulation/rnaseq/genome/arahy.Tifrunner.gnm1.KYV3.genome_main.fna stringtie_merged_se.gtf
#output: transcripts.fa is the fasta file of all assembles transcripts.    

##get non_gene_model candidate list
awk '{print $5}' non_gene_model > non_gene_model_list
##extract noncoding sequence       
module load kent
faSomeRecords transcripts.fa non_gene_model_list non_gene_model_stringtie.fasta
##use fasta_handle.py to remove redundant sequence and filter <200nt
python fasta_handle.py -i non_gene_model_stringtie.fasta -o non_gene_model_stringtie200_se.fasta -c -l 200
# -c is to remove redundant sequence 
# -l 200 is to filter sequence smaller than 200

##(1)prediction using CPC2
install CPC2
wget http://cpc2.cbi.pku.edu.cn/data/CPC2-beta.tar.gz
tar -zxvf CPC2-beta.tar.gz
cd CPC2-beta
export CPC_HOME="$PWD"
cd libs/libsvm
tar -zxvf libsvm-3.18.tar.gz
cd libsvm-3.18
make clean && make
##load the prerequisite modules by loading hpg preinstalled python version
module load python
python2.7 ./bin/CPC2.py -i non_gene_model_stringtie200_se.fasta -o CPC_se_result.txt

##(2)prediction using CPAT
module load gcc
module load cpat
##make your own hexamer, here used Arabidopsis gene CDS to build
make_hexamer_tab.py -c Araport11_genes.201606.cds.fasta -n plant_uniq_lnc.fasta > Plant_Hexamer.tsv
# -c must be CDS sequence (from start codon to stop codon)
# -n noncoding sequence
##Build logistic regression model (“prefix.logit.RData”) required by CPAT
#This program will output 3 files:
#prefix.feature.xls: A table contains features calculated from training datasets (coding and noncoding gene lists).
#prefix.logit.RData: logit model required by CPAT (if R was installed).
#prefix.make_logitModel.r: R script to build the above logit model.
make_logitModel.py -x Plant_Hexamer.tsv -c Araport11_genes.201606.cds.fasta -n plant_uniq_lnc.fasta -o Plant
##run main program using data used for building model, then use the prediction result to determine cutoff
cat Araport11_genes.201606.cds.fasta plant_uniq_lnc.fasta >test.fasta
cpat.py -g test.fasta -d Plant.logit.RData -x Plant_Hexamer.tsv -o test
# -g GENE_FILE, fasta file of your testing genes
# -d LOGIT_MODEL, Prebuilt training model. Run ‘make_logitModel.py’ to build logit model out of your own training datset
# -x HEXAMER_DAT, Run ‘make_hexamer_tab.py’ to make this table out of your own training dataset.

Rscript 10Fold_CrossValidation.r
##or just run the known lncRNA database to determine cutoff
cpat.py -g plant_uniq_lnc.fasta -d Plant.logit.RData -x Plant_Hexamer.tsv -o test
##run your transcripts
cpat.py -g non_gene_model_stringtie200.fasta -d Plant.logit.RData -x Plant_Hexamer.tsv -o peanut_cpat_result

##(3)blast against plant lncrna data
module load ncbi_blast
formatdb -i plant_uniq_lnc.fasta -p f
blastall -a 8 -p blastn -d plant_uniq_lnc.fasta -i ../non_gene_model_stringtie200.fasta -o peanut_lnc_blast_pe_result -e 1e-6 -m 9





##DGCA for differential correlation
R
#--------------------processing in R----------------------------------------------------------
test_se = read.csv('DEG_count_se.csv', header= T, row.names=1)    #expression data
design_se = read.csv('se_design.csv', row.names = 1)              #experiment design data
design_se = as.matrix(design_se)
head(design_se)
           X10_dpg X1_dpi X12_dpi X28_dpi X4_dpi X8_dpi
SRR5572304       1      0       0       0      0      0
SRR5572303       1      0       0       0      0      0
SRR5572302       1      0       0       0      0      0
SRR5572305       0      1       0       0      0      0
SRR5572306       0      1       0       0      0      0
SRR5572307       0      1       0       0      0      0

##create subset data for each comparison(treatment vs. control). i is in the range of comparison number
for (i in 1:5){assign(paste("design_se_",i,sep=""),design_se[c(1:3,(i*3+1):(i*3+3)),c(1,i+1)])}
for (i in 1:5){assign(paste("test_se_",i, sep=""),test_se[,c(rownames(get(paste("design_se_",i,sep=""))))])}
ddcor_res1 = ddcorAll(inputMat = test_se_1, design = design_se_1, compare = c("X10_dpg","X1_dpi"), adjust = "bonferroni")
ddcor_res2 = ddcorAll(inputMat = test_se_2, design = design_se_2, compare = c("X10_dpg","X12_dpi"), adjust = "bonferroni")
ddcor_res3 = ddcorAll(inputMat = test_se_3, design = design_se_3, compare = c("X10_dpg","X28_dpi"), adjust = "bonferroni")
ddcor_res4 = ddcorAll(inputMat = test_se_4, design = design_se_4, compare = c("X10_dpg","X4_dpi"), adjust = "bonferroni")
ddcor_res5 = ddcorAll(inputMat = test_se_5, design = design_se_5, compare = c("X10_dpg","X8_dpi"), adjust = "bonferroni")
#add splitSet = "geneID_of_interest" to ddcorAll() function to output gene pairs related to your gene of interest.

write.csv(ddcor_res1, 'se_DCNA_result_1vsC.csv')
write.csv(ddcor_res2, 'se_DCNA_result_12vsC.csv')
write.csv(ddcor_res3, 'se_DCNA_result_28vsC.csv')
write.csv(ddcor_res4, 'se_DCNA_result_4vsC.csv')
write.csv(ddcor_res5, 'se_DCNA_result_8vsC.csv')

#plot specific gene pairs that you found to be interesting
plotCors(inputMat = test_se_1, design = design_se_1, compare = c("X10_dpg","X1_dpi"), geneA = "gene1", geneB = "gene2")
#--------------------------------------------------------------------------------------------------


#——————————————————————enrichment analysis-------------------
# optional: using clusterProfiler package in R
# module load R
# R
# #get annotation information from OrgDb
# require(AnnotationHub)       #loading requied package for AnnotationHub
# query(hub,"Arachis hypogaea")
# peanut = hub[["AH72127"]]
# length(keys(peanut))[1]     #take a look at number of genes been annotated
# columns(peanut)        # information contained in the annotation
# #use Biological Id TRanslator: bitr(geneID, fromType, toType, OrgDb, drop = TRUE) to transfrom IDs
# require(clusterProfiler)


##extract the GO id for each gene using gff3 genome annotation file
python extract_GO_from_gff.py genes.gff3
#output: gene_GO_from_gff3.csv




#--------------------------------------------------------





