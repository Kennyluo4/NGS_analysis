
#check reads contanmination 
module load fastq-screen
fastq_screen --aligner bowtie2 --conf fastq_screen.conf S6121_ahy_1P.fq.gz


############
\filter rRNA 
############
module load sortmerna
for f in *_1.fq;do echo ${f} ${f%_*}_2.fq; merge-paired-reads.sh ${f} ${f%_*}_2.fq ${f%_*}_merged.fq; done

for f in S6-H*; do echo ${f}; sortmerna -a 4 -m 10240 --paired_out --log --num_alignments 1 --fastx --ref ~/ufrc/nodulation/genome/rrna/peanut_rRNA_final.fasta,/home/luoziliang/ufrc/nodulation/genome/rrna/peanut_rrna_index --reads ${f} --aligned ./${f%%.*}_rrna.fq --other ./${f%%.*}_peanut_merged.fq;done
for f in S6-2*; do echo ${f}; sortmerna -a 4 -m 10240 --paired_out --log --num_alignments 1 --fastx --ref ~/ufrc/nodulation/genome/rrna/peanut_rRNA_final.fasta,/ufrc/wang/luoziliang/nodulation/genome/rrna/peanut_rrna_index --reads ${f} --aligned ./${f%%.*}_rrna.fq --other ./${f%%.*}_peanut_merged.fq;done
for f in S6-12*; do echo ${f}; sortmerna -a 4 -m 10240 --paired_out --log --num_alignments 1 --fastx --ref ~/ufrc/nodulation/genome/rrna/peanut_rRNA_final.fasta,/home/luoziliang/ufrc/nodulation/genome/rrna/peanut_rrna_index --reads ${f} --aligned ./${f%%.*}_rrna.fq --other ./${f%%.*}_peanut_merged.fq;done
for f in S7-H*; do echo ${f}; sortmerna -a 4 -m 10240 --paired_out --log --num_alignments 1 --fastx --ref ~/ufrc/nodulation/genome/rrna/peanut_rRNA_final.fasta,/home/luoziliang/ufrc/nodulation/genome/rrna/peanut_rrna_index --reads ${f} --aligned ./${f%%.*}_rrna.fq --other ./${f%%.*}_peanut_merged.fq;done
for f in S7-2*; do echo ${f}; sortmerna -a 4 -m 10240 --paired_out --log --num_alignments 1 --fastx --ref ~/ufrc/nodulation/genome/rrna/peanut_rRNA_final.fasta,/home/luoziliang/ufrc/nodulation/genome/rrna/peanut_rrna_index --reads ${f} --aligned ./${f%%.*}_rrna.fq --other ./${f%%.*}_peanut_merged.fq;done
for f in S7-12*; do echo ${f}; sortmerna -a 4 -m 10240 --paired_out --log --num_alignments 1 --fastx --ref ~/ufrc/nodulation/genome/rrna/peanut_rRNA_final.fasta,/home/luoziliang/ufrc/nodulation/genome/rrna/peanut_rrna_index --reads ${f} --aligned ./${f%%.*}_rrna.fq --other ./${f%%.*}_peanut_merged.fq;done

for i in *peanut_merged.fq; do unmerge-paired-reads.sh ${i} ${i%%_*}_1P.fq ${i%%_*}_2P.fq; done

############
\alignment 
############
#1 hisat2 (choose either one)
module load hisat2 samtools python
for f in *1P.fq; do echo ${f} ${f%_*}_2P.fq; hisat2 -p 8 -x /home/luoziliang/ufrc/nodulation/genome/tif_gnm --known-splicesite-infile /home/luoziliang/ufrc/nodulation/rnaseq/genome/splice_sites.gtf -1 ${f} -2 ${f%_*}_2P.fq -S ${f%_*}.sam; samtools sort -@ 8 -o ${f%_*}.bam ${f%_*}.sam; echo finished\t${f%%_*}; done
for f in *.sam; do samtools sort -@ 8 -o ${f%.*}.bam ${f}; echo finished ${f};done

#2 STAR alignment
module load star
STAR --runThreadN 8 --genomeDir /ufrc/wang/luoziliang/nodulation/genome/star_index/ --outFilterScoreMinOverLread 0.3 --outFilterMatchNminOverLread 0.3 --outFileNamePrefix 6H1 --readFilesIn ../S6H1_ahy_1P.fq ../S6H1_ahy_2P.fq --sjdbGTFfile /ufrc/wang/luoziliang/nodulation/genome/arahy.Tifrunner.gnm1.ann1.CCJH.gene_models_main.gff3 --twopassMode Basic --outSAMstrandField intronMotif --outSAMtype BAM Unsorted
STAR --runThreadN 8 --genomeDir /ufrc/wang/luoziliang/nodulation/genome/star_index/ --outFilterScoreMinOverLread 0.3 --outFilterMatchNminOverLread 0.3 --outFileNamePrefix 6121 --readFilesIn ../S6121_ahy_1P.fq ../S6121_ahy_2P.fq --sjdbGTFfile /ufrc/wang/luoziliang/nodulation/genome/arahy.Tifrunner.gnm1.ann1.CCJH.gene_models_main.gff3 --twopassMode Basic --outSAMstrandField intronMotif --outSAMtype BAM Unsorted
STAR --runThreadN 8 --genomeDir /ufrc/wang/luoziliang/nodulation/genome/star_index/ --outFilterScoreMinOverLread 0.3 --outFilterMatchNminOverLread 0.3 --outFileNamePrefix 7H4 --readFilesIn ../S7H4_ahy_1P.fq ../S7H4_ahy_2P.fq --sjdbGTFfile /ufrc/wang/luoziliang/nodulation/genome/arahy.Tifrunner.gnm1.ann1.CCJH.gene_models_main.gff3 --twopassMode Basic --outSAMstrandField intronMotif --outSAMtype BAM Unsorted
STAR --runThreadN 8 --genomeDir /ufrc/wang/luoziliang/nodulation/genome/star_index/ --outFilterScoreMinOverLread 0.3 --outFilterMatchNminOverLread 0.3 --outFileNamePrefix 7H1 --readFilesIn ../S7H1_ahy_1P.fq ../S7H1_ahy_2P.fq --sjdbGTFfile /ufrc/wang/luoziliang/nodulation/genome/arahy.Tifrunner.gnm1.ann1.CCJH.gene_models_main.gff3 --twopassMode Basic --outSAMstrandField intronMotif --outSAMtype BAM Unsorted
#do it for all samples, submit separately for faster run

#sort samples
for f in *.sam; do samtools sort -@ 8 -o ${f%.*}.bam ${f}; echo finished ${f};done

####################
\stringtie assembly
###################
module load stringtie
for f in *.bam; do echo ${f}; stringtie -p 8 -f 0.3 -j 5 -G /home/luoziliang/ufrc/nodulation/genome/arahy.Tifrunner.gnm1.ann1.CCJH.gene_models_main.gff3 -o ${f%.*}.gtf ${f}; done
# -p: number of threads/cores to use for the software(default: 1)
# -f <0.0-1.0>	Sets the minimum isoform abundance of the predicted transcripts as a fraction of the most abundant transcript assembled at a given locus. Lower abundance transcripts are often artifacts of incompletely spliced precursors of processed transcripts. Default: 0.1
# -j <float>	There should be at least this many spliced reads that align across a junction (i.e. junction coverage). This number can be fractional, since some reads align in more than one place. A read that aligns in n places will contribute 1/n to the junction coverage. Default: 1
# -c <float>	Sets the minimum read coverage allowed for the predicted transcripts. A transcript with a lower coverage than this value is not shown in the output. Default: 2.5
# -g <int>	Minimum locus gap separation value. Reads that are mapped closer than this distance are merged together in the same processing bundle. Default: 50 (bp)
# -M <0.0-1.0>	Sets the maximum fraction of muliple-location-mapped reads that are allowed to be present at a given locus. Default: 0.95
stringtie --merge -p 8 -T 1 -G /home/luoziliang/ufrc/nodulation/genome/NCBI_arahy.Tifrunner.gnm1.KYV3.gff3 -o stringtie_merged.gtf ./mergelist.txt
# -T <min_tpm>	minimum input transcript TPM to include in the merge (default: 0)

# #merge all bam files to assemble. Because the merged file is too large, split by chromosome. Then assemble for individual chromosomes (merge all scaffold)
# #(1)split bam file
# module load bamtools
# bamtools split -in merged.bam -reference
# #-reference: to split by chromosomes
# #(2)merge scaffolds
# samtools merge -@ 20 merged.REF_arahy.Tifrunner.gnm1.All_scaffold.bam merged.REF_arahy.Tifrunner.gnm1.scaffold*.bam
# #(3)assemble each chromosomes
# for f in merged.REF_arahy.Tifrunner.gnm1.scaffold*.bam; do echo ${f}; stringtie -p 20 -G /home/luoziliang/ufrc/nodulation/genome/arahy.Tifrunner.gnm1.ann1.CCJH.gene_models_main.gff3 -l STRG${f:41:3} -o ${f%.*}.gtf ${f}; done
# for f in merged.REF_arahy.Tifrunner.gnm1.Arahy.0*.bam; do echo ${f} start at $(date +"%T") >>stringtielog; stringtie -p 20 -G /home/luoziliang/ufrc/nodulation/genome/arahy.Tifrunner.gnm1.ann1.CCJH.gene_models_main.gff3 -l STRG${f:38:2} -o ${f%.*}.gtf ${f}; echo end $(date +"%T")>>stringtielog ;done


#check the number of annotated transcripts
awk '$3=="transcript"' stringtie_merged.gtf | wc -l
188266
# Ze assembled trancript number: 188018 

#compare assembled transcript with genome annotation file
module load gffread
gffcompare -r /home/luoziliang/ufrc/nodulation/genome/NCBI_arahy.Tifrunner.gnm1.KYV3.gff3 -G -o merged_gtf stringtie_merged.gtf

#check the number of annotated transcripts
awk '$3=="transcript"' merged_gtf.annotated.gtf |wc -l
188225
#notice that the number are slightly less, some overlapped assembled transcripts are ignored.

#exclude chroloplast chromosomes
grep -v "Pltd" merged_gtf.annotated.gtf >merged_gtf.annotated.final.gtf

#append gene name to gene ID
#check to see which term (refgeneID or gene_name) you want to append to merged ID
#modify the mstrg_gtf_file_prep.py script before running it
python3 mstrg_gtf_file_prep.py merged_gtf.annotated.final.gtf> merged_gtf.annotated_IDmodified.gtf
#only genes with overlapped transcripts who have class code of =, j, e, s, o will be append to the MSTRG gene ID. Transcripts with other class are unlikely the ref gene or ref gene isoform

#check the number of transcripts assembled:
awk '$3=="transcript"' merged_gtf.annotated_IDmodified.gtf | wc -l
128495

for f in `ls ../*.bam`; do echo ${f}; stringtie -e -B -p 20 -G merged_gtf.annotated_IDmodified.gtf -o ./ballgown/${f%.*}/${f%.*}.gtf ${f}; done
#using tximport to correct changes in gene lenghths across samples and increase sensitivity (by avoid discarding multi aligned reads)

#extract read count
python prepDE.py

#get sequence of transcirpts
gffread -w transcripts.fa -g /ufrc/wang/luoziliang/nodulation/genome/arahy.Tifrunner.gnm1.KYV3.genome_main.fna merged_gtf.annotated_IDmodified.gtf

#####trinity de novo assembly
# module load trinity
# Trinity --seqType fq --SS_lib_type RF --max_memory 40G --min_kmer_cov 1 --CPU 10 --samples_file reads_file_list --output trinity_test

# #examine the statistic for the Trinity assemblies
# TrinityStats trinity_out_dir/Trinity.fasta

# #align assembled transcripts to reference transcriptome, and examine the length coverage of top database hits
# blastn
# analyze_blastPlus_topHit_coverage.pl \Trinity_vs_S_pombe_genes.blastn \trinity_out_dir/Trinity.fasta \S_pombe_refTrans.fasta

###############################
\Trinity genome-guided assembly
###############################
#merge all the aligned reads
module load samtools trinity
samtools merge -b bam_list -O BAM -@ 10 merged.bam
samtools merge -b bam_e6 -O BAM -@ 20 merged_E6.bam
samtools merge -b bam_e7 -O BAM -@ 20 merged_E7.bam
#assemble
Trinity --genome_guided_bam merged.bam --CPU 10 --max_memory 100G --genome_guided_max_intron 10000
#the merged bam file is too large to be proceesed.
#split the merged file by chromosome to do the assembly
module load bamtools
bamtools split -in merged.bam -reference

#########################
\ortholog identification
#########################
#predict the protein sequence from the transcript sequence
#Check how many transcripts
grep ">" transcripts.fa | wc -l
188097

# #(1)use OrfPredictor
# perl OrfPredictor/OrfPredictor.pl transcripts.fa blast_demo 0 both 1e-5 predicted_peptide.fa
# #count how many peptides predicted 
# grep ">" predicted_peptide.fa | wc -l
# 128473
# #I don't think this is convincing that almost every transcripts can be translated.

#(2)use Transdecoder
module load transdecoder
TransDecoder.LongOrfs -t transcripts.fa
#(optional)use protein database (swissprot and pfam) blast information. This step takes long time
module load ncbi_blast pfamscan
blastp -query transcripts.fa.transdecoder_dir/longest_orfs.pep -db swissprot -max_target_seqs 1 -outfmt 6 -evalue 1e-5 -num_threads 26 > blastp.outfmt6
#(optional)download pfam database
wget ftp://ftp.ebi.ac.uk/pub/databases/Pfam/releases/Pfam33.0/Pfam-A.hmm.gz
hmmpress Pfam-A.hmm 
#(optional)scan the database
hmmscan --cpu 26 --domtblout pfam.domtblout Pfam-A.hmm transcripts.fa.transdecoder_dir/longest_orfs.pep
#(optional)Integrating the Blast and Pfam search results into coding region selection
TransDecoder.Predict -t transcripts.fa --retain_pfam_hits pfam.domtblout --retain_blastp_hits blastp.outfmt6 --single_best_only
#if don't need database guided prediction. simply run: TransDecoder.Predict -t transcripts.fa
grep ">" predicted_peptide.fa | wc -l

#used transdecoder predicted

# ####
# \orthomcl
# #rename file as speciesname.fasta, and name each protein as >speciesname|genename
# mv transcripts.fa.transdecoder.pep ahy.fasta
# #modify the ID, add species name and remove extra annotations
# sed -i "s/>/>ahy|/g" ahy.fasta
# # sed -i "s/ GENE\..*//g" ahy.fasta
# orthomclFilterFasta complaintFasta/ 10 20
# #blast all to all
# module load ncbi_blast
# makeblastdb -dbtype prot -in goodProteins.fasta -out goodProteins.fasta
# blastall -a 8 -p blastp -d goodProteins.fasta -i goodProteins.fasta -o blastresults -e 1e-6 -m 8

# #parse blast result
# orthomclBlastParser blastresults complaintFasta/ >> similarSequences.txt
# cp $HPC_ORTHOMCL_CONF/orthomcl.config .
# vi orthomcl.config 
# #change the sql name in dbConnectString=  with a pattern 'orthomcl_$USER_project_name' to your own name


# #use these function to make a database
# hpc_list_orthomcl_databases
# hpc_create_orthomcl_database
# hpc_remove_orthomcl_database

# #install 
# orthomclInstallSchema orthomcl.config

# #run orthomcl
# orthomclLoadBlast orthomcl.config similarSequences.txt
# orthomclPairs orthomcl.config pairs.log cleanup=no

###orthomcl has many issues since it needs sql database
\orthofinder 
sed -i "s/>/>ahy|/g" ahy.fa
#only need to provide the sequence of each organisms. one file one organism, put all fasta file in one directory 
# remove any duplicate sequence and ID to avoid error
module load orthofinder
orthofinder -a 10 -M msa -f orthofinder/ 
#try -M msa if errors occoured 
#results:
	OrthoFinder assigned 106847 genes (97.6% of total) to 9662 orthogroups
#An orthogroup is the group of genes descended from a single gene in the LCA of a group of species
#The Orthogroups.txt are the groups of orthologs. each row is an orthogroup (group ID + orthologs belonging to this group)
#The Ortholougs directory include species pairwise orthologs. e.g. ahy_to_mtr, ahy_to_glm


#########################
\annotation of assembled trasncripts
#########################
#use Trinotate for transcript annotation. required files: transcripts sequence: transcripts.fa; predicted peptide sequence: transcripts.fa.transdecoder.pep
module load trinotate/3.2.0 ncbi_blast
#to get the databases needed for downstream analysis
Build_Trinotate_Boilerplate_SQLite_db.pl Peanut
makeblastdb -in uniprot_sprot.pep -dbtype prot
gunzip Pfam-A.hmm.gz
hmmpress Pfam-A.hmm
#blast the database using assembled transcripts (transcripts.fa) and translated peptides of these transcripts (transcripts.fa.transdecoder.pep)
blastx -query transcripts.fa -db uniprot_sprot.pep -num_threads 8 -max_target_seqs 1 -outfmt 6 -evalue 1e-3 > blastx.outfmt6
blastp -query transcripts.fa.transdecoder.pep -db uniprot_sprot.pep -num_threads 8 -max_target_seqs 1 -outfmt 6 -evalue 1e-3 > blastp.outfmt6

#Running HMMER to identify protein domains
module load hmmer
hmmscan --cpu 12 --domtblout TrinotatePFAM.out Pfam-A.hmm transcripts.fa.transdecoder.pep > pfam.log

#Running signalP to predict signal peptides
module load signalp/4.1
signalp -f short -n signalp.out transcripts.fa.transdecoder.pep

#Running tmHMM to predict transmembrane regions
module load trinotate
tmhmm --short < transcripts.fa.transdecoder.pep > tmhmm.out

# #Running RNAMMER to identify rRNA transcripts
# #the RNAMMER software is integrated into trinotate module, the RnammerTranscriptome.pl will process all the transcripts and call RNAMMER to do the job
# module load trinotate
# RnammerTranscriptome.pl --transcriptome transcripts.fa
# #it will generated a file: Trinity.fasta.rnammer.gff for downstream annotation
#!!!!the memory usage of this analysis is too large, Skip this step

#prepare GeneID - TranscriptID relationship file
#Gene/Transcript relationships (tab delimited format: "gene_id(tab)transcript_id"
python3 getTransGeneMap.py merged_gtf.annotated_IDmodified.gtf

#note transfer files from window to linux may cause issues because of coding difference. Check the file before use: cat -te gene_trans_map | less
#to solve this issue:
# cat transcripts.fa | perl -lane 's/\cM//g; print;' > transcripts.adj.fa
# cat transcripts.fa.transdecoder.pep | perl -lane 's/\cM//g; print;' > transcripts.fa.transdecoder.adj.pep
# cat gene_trans_map | perl -lane 's/\cM//g; print;'> gene_trans_map.adj

#Load transcripts and coding regions
Trinotate Peanut.sqlite init --gene_trans_map gene_trans_map --transcript_fasta transcripts.fa --transdecoder_pep transcripts.fa.transdecoder.pep
Trinotate Peanut.sqlite LOAD_swissprot_blastp blastp.outfmt6
Trinotate Peanut.sqlite LOAD_swissprot_blastx blastx.outfmt6
Trinotate Peanut.sqlite LOAD_pfam TrinotatePFAM.out
Trinotate Peanut.sqlite LOAD_tmhmm tmhmm.out
Trinotate Peanut.sqlite LOAD_signalp signalp.out

#generate the output
Trinotate Peanut.sqlite report --incl_pep --incl_trans -E 1e-4 > trinotate_annotation_report.xls
#  -E <float>                 maximum E-value for reporting best blast hit
#                             and associated annotations.
#							  Example: 1e-3
#  --pfam_cutoff <string>     'DNC' : domain noise cutoff (default)
#                             'DGC' : domain gathering cutoff
#                             'DTC' : domain trusted cutoff
#                             'SNC' : sequence noise cutoff
#                             'SGC' : sequence gathering cutoff
#                             'STC' : sequence trusted cutoff
# In the report file, backticks (`) and carets (^) are used as delimiters for data packed within an individual field,
# When there are multiple assignments in a given field, the assignments are separated by (`) and the fields within an assignment are separated by (^).

#take the annotation from GFF file 
grep -P "\t\w*\tmRNA|\t\w*\tpseudogene|\t\w*\tlnc_RNA|\t\w*\ttranscript|\t\w*\trRNA|\t\w*\tsnRNA|\t\w*\tsnoRNA|\t\w*\trRNA|\t\w*\ttRNA" NCBI_arahy.Tifrunner.gnm1.KYV3.gff3 > NCBI_annotation.txt
#process the ID and gene product in excel, make it name as NCBI_annotation.csv

#########################
\enrichment analysis for DEG
#########################
#1st step is to get all the GO and KEGG annotation for your trancripts and genes.
#Trinotate result provide the GO and KEGG annotation based on blast result from Uniprot and Pfam. But it's not very accurate.

###Use KAAS to annotate KEGG ID on:https://www.genome.jp/tools/kaas/
###download KOID with KOterm relationship using KEGG API. For more information on how to use:https://www.kegg.jp/kegg/rest/keggapi.html
http://rest.kegg.jp/list/ko
#replace the long annoation, and using abbriviation term for each KO
replace ;\s.*
E\d*\.\d*\.\d*\.\d*\w\, 
E\d*\.\d*\.\d*\.\d*\w\; 
\[EC\:\d*\.\d*\.\d*\.\d*.*\]
[paired donor for1 atom of oxygen]
#prepare files for Enrichment analysis
1. used python script RetriveAnnoFromTrinotate.py to grab the GOID and KEGGID for transcripts/genes from Trinotate result
2. optional, after generating the gene-GO/KEGG relationship csv file, if you have gff file with GOID, or get the keggID from KAAS, add to these files and remove redundancy.
# org used for KAAS search: hsa, dme, cel, sce, cho, eco, nme, hpy, rpr, bsu, lla, cac, mge, mtu, ctr, bbu, syn, bth, dra, aae, mja, ape, brp, bna, boe, gmx, gsj, mtr, vun, lja, adu, aip, lang, ath, osa, dosa, ats, zma, atr
3. download GO2term relationship from GO website (http://current.geneontology.org/ontology/go.obo), and extract the relationship(GOID GOterm)


#input into clusterProfier for enrichment analysis
#load the file in Select organism/annotation section
#Copy paste your gene list of interest into the board for enrichment analysis


###########################
#Alternative using BingGO to do enrichment
#User guide:https://www.psb.ugent.be/cbd/papers/BiNGO/User_Guide.html
1.BingGO do not update ortholog database, need to download the obo database:http://current.geneontology.org/ontology/go.obo.
#recommend to use plant Plant subset: goslim_plant.obo
2. create your own annotation file
#gene identifier = ontology category identifier,(KEGG identifier can also be used)
(species=Saccharomyces cerevisiae)(type=Biological Process)(curator=GO)
YAL001C = 0006384
YAL002W = 0045324



##################
\lncRNA analysis 
################
#get non-gene-models transcripts as potential lncRNA.
awk '{if ($3=="u" || $3=="x" || $3=="i" || $3=="j" || $3=="o" || $3=="class_code"){print $0}}' merged_gtf.stringtie_merged.gtf.tmap > non_gene_model
awk '{print $5}' non_gene_model > non_gene_model_list
#26169 non_gene_model in peanut annotation file

#get all trancript sequence
module load gffread
gffread -w transcripts.fa -g /ufrc/wang/luoziliang/nodulation/genome/arahy.Tifrunner.gnm1.KYV3.genome_main.fna merged_gtf.annotated_IDmodified.gtf
#get the sequence of non-gene-models

#blast to swissprot to further filter coding genes
module load ncbi_blast
blastx -query transcripts.fa -db swissprot -num_threads 8 -max_target_seqs 1 -outfmt 6 -evalue 1e-6 > non_gene_model_blast.result
# based on blast result, manually filter transcripts with a hit in database, update the non_gene_model_list
# after filtering, 21628 transcripts left

module load kent
faSomeRecords transcripts.fa non_gene_model_list non_gene_model_stringtie_transcripts.fa
#remove redundant sequence and filter transcripts>200nt
module load python
python fasta_handle.py -i non_gene_model_stringtie_transcripts.fa -o non_gene_model_stringtie_transcripts200.fa -c -l 200

#mv non_gene_model_stringtie_transcripts200.fa to lncRNA folder

#run lncRNA prediction using CPC2
module load python/2.7
python /ufrc/wang/luoziliang/CPC2-beta/bin/CPC2.py -i non_gene_model_stringtie_transcripts200.fa -o CPC_longRNA_result.5.25.2020
#run lncRNA prediction using CPAT2
module load gcc cpat
#the model has been built before for peanut 
# ##make your own hexamer, here used Arabidopsis gene CDS to build
# make_hexamer_tab.py -c Araport11_genes.201606.cds.fasta -n plant_uniq_lnc.fasta > Plant_Hexamer.tsv
# # -c must be CDS sequence (from start codon to stop codon)
# # -n noncoding sequence
# ##Build logistic regression model (“prefix.logit.RData”) required by CPAT
# #This program will output 3 files:
# #prefix.feature.xls: A table contains features calculated from training datasets (coding and noncoding gene lists).
# #prefix.logit.RData: logit model required by CPAT (if R was installed).
# #prefix.make_logitModel.r: R script to build the above logit model.
# make_logitModel.py -x Plant_Hexamer.tsv -c Araport11_genes.201606.cds.fasta -n plant_uniq_lnc.fasta -o Plant
# ##run main program using data used for building model, then use the prediction result to determine cutoff
# cat Araport11_genes.201606.cds.fasta plant_uniq_lnc.fasta >test.fasta
cpat.py -g non_gene_model_stringtie_transcripts200.fa -d ~/ufrc/CPAT/Peanut.logit.RData -x ~/ufrc/CPAT/Peanut_Hexamer.tsv -o cpat_long_rna_result.5.26.2020
#get the intersect of predicted noncoding list as:predicted_lncrna_list

#run blast against known lncRNA database
module load ncbi_blast
formatdb -i plant_uniq_lnc.fasta -p F
blastn -query non_gene_model_stringtie_transcripts200.fa -db plant_uniq_lnc.fasta -num_threads 16 -max_target_seqs 1 -outfmt 6 -evalue 1e-6 > blast_lnc_database.result
# a total of 16380 (union) lnRNA

#filter the known genes in NCBI annotation file.
# append the NCBI peanut lncRNA list to the predicted lnc list, to search if there is match: result- none of the NCBI lncRNA is predicted by above method
# append the assembled transcripts-appened annotation to lnclist
# for not novel trasncripts (Don't start with MSTRG), see if they are protein coding gene (annotation contain "uncharacterized", does not contain"protein").
# tag the known transcript ID with "uncharacterized" after manual curation  (91 uncharacterized)
# tag the novel transcript ("MSTRG.x.x") with "uncharacterized" (9619)
# add the "uncharacterized" lncRNA with NCBI annotated lncRNA (8587) to get the final lncRNA list
# final reliable lncRNA number: 18297   

module load kent
faSomeRecords non_gene_model_stringtie_transcripts200.fa predicted_lncrna_list predicted_lncrna.fa


####################################################
\WGCNA analysis 
####################################################
# use DESeq2 normalized count data as input counts(dds, normalized=TRUE)
# combine the normalized data of different RNA species 

data$variance = apply(data, 1, var)
data2 = data[data$variance >= quantile(data$variance, c(.50)), ] #50% most variable genes
data2$variance <- NULL





#############################################
\leafcutter for alternative splicing analysis
#############################################
export PATH=/home/luoziliang/ufrc/nodulation/rnaseq/longRNA/clean_peanut_reads/leafcutter/scripts:$PATH
for bamfile in *.bam; do echo Converting $bamfile to $bamfile.junc; sh bam2junc.sh $bamfile $bamfile.junc; echo $bamfile.junc >> test_juncfiles.txt; done

export PATH=/home/luoziliang/ufrc/nodulation/rnaseq/longRNA/clean_peanut_reads/leafcutter/clustering:$PATH
leafcutter_cluster.py -j test_juncfiles.txt -m 50 -o testCompare -l 500000

####notes
Gene quantification

With regards to gene-level quantification it is now widely recognized that performing transcript quantification and afterwards obtaining the gene expression by adding together the expression from the individual transcripts will result in improved gene-level analysis. Prime examples illustrating concept is this blogpost and this article. The “To Long - Did not Read” is that compared to the old-school gene-quantification (genome-mapping + featureCounts/HTSeq) the main advantages of the transcript-level gene quantification are:

The inclusion of multi-mappers in the quantification (ignored by featureCounts/HTSeq) which typically correspond to 20-50% of all reads in a sample that would otherwise be ignored!.
The more accurate quantification algorithm (which among others include correction of various types of sequence-biases).
The ability to correctly quantify genes even if they contain isoform switches (if there is a switch the old-school way of doing it will give wrong quantifications).
The runtime. Lightweight algorithms are much faster!



####################################################
\use public computing resource for analysis: galaxy
####################################################
#upload your data from local server to distant galaxy server
module load ubuntu
lftp -u luoziliang@ufl.edu,kv8565356 usegalaxy.org
#after connected to the galaxy server lftp luoziliang@ufl.edu@usegalaxy.org:~>
#use put function to put target files to galaxy server
put merged.bam



